
#%%
from cgitb import reset
from turtle import forward, shape
from numpy import percentile
import torch.nn as nn
import torch.nn.functional as F
from models.NF import MAF
import torch

def interpolate(tensor, index, target_size, mode = 'nearest', dim = 0):
    print(tensor.shape)
    source_length = tensor.shape[dim]
    if source_length > target_size:
        raise AttributeError('no need to interpolate')
    if dim == -1:
        new_tensor = torch.zeros((*tensor.shape[:-1], target_size),dtype=tensor.dtype, device=tensor.device)
    if dim == 0:
        new_tensor = torch.zeros((target_size, *tensor.shape[1:], ),dtype=tensor.dtype, device=tensor.device)
    scale = target_size // source_length
    reset = target_size % source_length
    # if mode == 'nearest':
    new_index = index
    new_tensor[new_index, :] = tensor
    new_tensor[:new_index[0], :] = tensor[0,:].unsqueeze(0)
    for i in range(source_length-1):
        new_tensor[new_index[i]:new_index[i+1] , :] = tensor[i,:].unsqueeze(0)
    new_tensor[new_index[i+1] :,:] = tensor[i+1,:].unsqueeze(0)
    return new_tensor

class GNN(nn.Module):
    """
    The GNN module applied in GANF
    """
    def __init__(self, input_size, hidden_size):

        super(GNN, self).__init__()
        self.lin_n = nn.Linear(input_size, hidden_size)
        self.lin_r = nn.Linear(input_size, hidden_size, bias=False)
        self.lin_2 = nn.Linear(hidden_size, hidden_size)

    def forward(self, h, A):
        ## A: K Â· K
        ## H: N Â· K  Â· L Â· D
        # print(h.shape, A.shape)
        # h_n = self.lin_n(torch.einsum('nkld,kj->njld',h,A))
        # h_n = self.lin_n(torch.einsum('nkld,kj->njld',h,A))
        # print(h.shape, A.shape)
        h_n = self.lin_n(torch.einsum('nkld,nkj->njld',h,A))
        '''
        è·å–çª—å£ä¸‹æ‰€æœ‰tæ—¶åˆ»åŠå…¶å†å²ä¿¡æ¯çš„æ—¶ç©ºæ¡ä»¶ï¼š
        h_rï¼šåªè€ƒè™‘åºåˆ—ä¸­é™¤äº†æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„æ‰€æœ‰æ—¶é—´æ­¥â€”â€”>å®ä½“æœ¬èº«çš„å†å²ä¿¡æ¯æœ‰åŠ©äºå¢å¼ºæ—¶é—´åºåˆ—çš„æ—¶é—´å…³ç³»
        h_nï¼šå°†é€’å½’ç‰¹å¾æ·»åŠ åˆ° h_n çš„ç›¸åº”ä½ç½®ï¼Œä»ç¬¬äºŒä¸ªæ—¶é—´æ­¥å¼€å§‹
        å®ç°äº†tæ—¶åˆ»å’Œt-1æ—¶åˆ»çš„å†å²ä¿¡æ¯ç›¸åŠ ï¼Œå®ç°å…¬å¼11
        '''
        h_r = self.lin_r(h[:,:,:-1])
        h_n[:,:,1:] += h_r
        h = self.lin_2(F.relu(h_n))

        return h

import math
import torch.nn as nn
import matplotlib.pyplot as plt
def plot_attention(data, i, X_label=None, Y_label=None):
  '''
    Plot the attention model heatmap
    Args:
      data: attn_matrix with shape [ty, tx], cutted before 'PAD'
      X_label: list of size tx, encoder tags
      Y_label: list of size ty, decoder tags
  '''
  fig, ax = plt.subplots(figsize=(20, 8)) # set figure size
  heatmap = ax.pcolor(data, cmap=plt.cm.Blues, alpha=0.9)
  fig.colorbar(heatmap)
  # Set axis labels
  if X_label != None and Y_label != None:
    X_label = [x_label for x_label in X_label]
    Y_label = [y_label for y_label in Y_label]
    
    xticks = range(0,len(X_label))
    ax.set_xticks(xticks, minor=False) # major ticks
    ax.set_xticklabels(X_label, minor = False, rotation=45)   # labels should be 'unicode'
    
    yticks = range(0,len(Y_label))
    ax.set_yticks(yticks, minor=False)
    ax.set_yticklabels(Y_label[::-1], minor = False)   # labels should be 'unicode'
    
    ax.grid(True)
    plt.show()
    plt.savefig('graph/attention{:04d}.jpg'.format(i))



class ScaleDotProductAttention(nn.Module):
    """
    compute scale dot product attention

    Query : given sentence that we focused on (decoder)
    Key : every sentence to check relationship with Qeury(encoder)
    Value : every sentence same with Key (encoder)
    """

    def __init__(self, c):
        super(ScaleDotProductAttention, self).__init__()
        self.w_q = nn.Linear(c, c)
        self.w_k = nn.Linear(c, c)
        self.w_v = nn.Linear(c, c)
        self.softmax = nn.Softmax(dim = 1)
        self.dropout = nn.Dropout(0.2)
        # swat_0.2
    def forward(self, x,mask=None, e=1e-12):
        # input is 4 dimension tensor
        # [batch_size, head, length, d_tensor]
        shape = x.shape
        x_shape = x.reshape((shape[0],shape[1], -1))
        batch_size, length, c = x_shape.size()
        q = self.w_q(x_shape)
        k = self.w_k(x_shape)
        k_t = k.view(batch_size, c, length)  # transpose
        score = (q @ k_t) / math.sqrt(c)  # scaled dot product

        # 2. apply masking (opt)
        if mask is not None:
            score = score.masked_fill(mask == 0, -1e9)

        # 3. pass them softmax to make [0, 1] range
        score = self.dropout(self.softmax(score))



        return score, k


class MTGFLOW(nn.Module):

    def __init__ (self, n_blocks, input_size, hidden_size, n_hidden, window_size, n_sensor, dropout = 0.1, model="MAF", batch_norm=True):
        super(MTGFLOW, self).__init__()

        self.rnn = nn.LSTM(input_size=input_size,hidden_size=hidden_size,batch_first=True, dropout=dropout)
        self.gcn = GNN(input_size=hidden_size, hidden_size=hidden_size)
        if model=="MAF":
            # self.nf = MAF(n_blocks, n_sensor, input_size, hidden_size, n_hidden, cond_label_size=hidden_size, batch_norm=batch_norm,activation='tanh', mode = 'zero')
            self.nf = MAF(n_blocks, n_sensor, input_size, hidden_size, n_hidden, cond_label_size=hidden_size, batch_norm=batch_norm,activation='tanh')
      
        self.attention = ScaleDotProductAttention(window_size*input_size)
    def forward(self, x, ):

        return self.test(x, ).mean()

    def test(self, x, ):
        # x: N Â· K Â· L Â· D 
        # N è¡¨ç¤ºbatchä¸­çš„æ ·æœ¬æ•°é‡
        # ğ¾ è¡¨ç¤ºæ¯ä¸ªæ ·æœ¬çš„å®ä½“æ€»æ•°
        # ğ¿ è¡¨ç¤ºæ¯ä¸ªå®ä½“çš„è§‚æµ‹æ€»æ•°
        # D è¡¨ç¤ºæ¯ä¸ªè§‚æµ‹çš„ç‰¹å¾æ•°é‡
        # x shape [batch_size, head, length, d_tensor]
        full_shape = x.shape # æœ‰4ä¸ªç»´åº¦ï¼ŒN K L D
        # print(full_shape)
        graph,_ = self.attention(x) #è¿”å›scoreï¼Œkeyï¼Œå­¦ä¹ é‚»å±…çŸ©é˜µA
        self.graph = graph
        # reshape: N*K, L, D
        x = x.reshape((x.shape[0]*x.shape[1], x.shape[2], x.shape[3]))
        h,_ = self.rnn(x) # åªä¿ç•™æ•´ä¸ªåºåˆ—çš„ä¿¡æ¯ hï¼ŒhåŒ…å«äº†è¾“å…¥åºåˆ—xçš„æ—¶é—´åŠ¨æ€ä¿¡æ¯
        '''
        h æ˜¯ LSTM å±‚çš„è¾“å‡ºï¼Œå®ƒæ˜¯ä¸€ä¸ªä¸‰ç»´å¼ é‡ï¼Œå…¶å½¢çŠ¶ä¸º (num_layers * num_directions, batch_size, hidden_size)ã€‚
        num_layers æ˜¯ LSTM ç½‘ç»œçš„å±‚æ•°ã€‚
        num_directions è¡¨ç¤º LSTM æ˜¯å¦æ˜¯åŒå‘çš„ï¼Œå•å‘ä¸º 1ï¼ŒåŒå‘ä¸º 2ã€‚
        hidden_size æ˜¯ LSTM å±‚çš„éšè—çŠ¶æ€å¤§å°ï¼Œè¿™ä¸ªå¤§å°é€šå¸¸ç”±æ¨¡å‹è®¾è®¡è€…æŒ‡å®šï¼Œç”¨äºæ•æ‰è¾“å…¥æ•°æ®çš„é«˜çº§ç‰¹å¾è¡¨ç¤ºã€‚
        '''

        # resahpe: N, K, L, H
        h = h.reshape((full_shape[0], full_shape[1], h.shape[1], h.shape[2]))# æŠŠnum_layers * num_directionsä¸¢æ‰ï¼Œä¸éœ€è¦
        h = self.gcn(h, graph)# xçš„ç‰¹å¾ç»è¿‡LSTMæå–åï¼Œç”¨hæ›¿æ¢æ‰xçš„ç‰¹å¾ã€‚æ–°çš„xä¸é‚»å±…çŸ©é˜µAå·ç§¯â€”â€”>å®ç°å…¬å¼11

        # reshappe N*K*L,H
        h = h.reshape((-1,h.shape[3]))# å±•å¹³ä¸ºäºŒç»´æ•°æ®ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯æ‰¹æ¬¡ä¸­çš„å…ƒç´ æ€»æ•°ï¼Œç¬¬äºŒä¸ªç»´åº¦æ˜¯æ¯ä¸ªå…ƒç´ çš„LSTMåçš„ç‰¹å¾æ•°ã€‚
        # reshappe N*K*L,D
        x = x.reshape((-1,full_shape[3]))# å±•å¹³ä¸ºäºŒç»´æ•°æ®ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯æ‰¹æ¬¡ä¸­çš„å…ƒç´ æ€»æ•°ï¼Œç¬¬äºŒä¸ªç»´åº¦æ˜¯æ¯ä¸ªå…ƒç´ LSTMå‰çš„ç‰¹å¾æ•°ã€‚
        '''
        #è¯¥æ–¹æ³•è®¡ç®—äº†è¾“å…¥ x å’Œ h çš„å¯¹æ•°æ¦‚ç‡
        ç„¶åä½¿ç”¨MAFè¿›è¡Œå½’ä¸€åŒ–æµ
        full_shape[1]ï¼šæ˜¯å®ä½“ï¼ˆçª—å£ï¼‰çš„æ€»æ•°
        full_shape[2]ï¼šæ˜¯log_probçš„window_size
        hï¼šæ˜¯LSTMæå–çš„ç‰¹å¾ï¼Œæ˜¯y
        vvvvvvvvvvvvvvvvvvvvv'''
        # TODO
        # log_prob = self.nf.log_prob(x, full_shape[1], full_shape[2], h).reshape([full_shape[0],-1])
        log_prob = self.nf.log_prob(x, full_shape[1], full_shape[2]).reshape([full_shape[0],-1])
        # log_probé‡å¡‘ä¸ºä¸€ä¸ªäºŒç»´å¼ é‡ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯æ‰¹æ¬¡å¤§å°ï¼Œç¬¬äºŒä¸ªç»´åº¦æ˜¯è‡ªåŠ¨è®¡ç®—çš„ã€‚ä»¥ä¾¿å¯ä»¥è¿›ä¸€æ­¥å¤„ç†æˆ–ç”¨äºæŸå¤±è®¡ç®—
        log_prob = log_prob.mean(dim=1)

        return log_prob

    def get_graph(self):
        return self.graph

    def locate(self, x, ):
        # x: N X K X L X D 
        full_shape = x.shape

        graph, _ = self.attention(x)
        # reshape: N*K, L, D
        self.graph = graph
        x = x.reshape((x.shape[0]*x.shape[1], x.shape[2], x.shape[3]))
        h,_ = self.rnn(x)

        # resahpe: N, K, L, H
        h = h.reshape((full_shape[0], full_shape[1], h.shape[1], h.shape[2]))
        h = self.gcn(h, graph)

        # reshappe N*K*L,H
        h = h.reshape((-1,h.shape[3]))
        x = x.reshape((-1,full_shape[3]))
        a = self.nf.log_prob(x, full_shape[1], full_shape[2], h)
        log_prob, z = a[0].reshape([full_shape[0],full_shape[1],-1]), a[1].reshape([full_shape[0],full_shape[1],-1])
        


        return log_prob.mean(dim=2), z.reshape((full_shape[0]* full_shape[1],-1))


class test(nn.Module):
    def __init__ (self, n_blocks, input_size, hidden_size, n_hidden, window_size, n_sensor, dropout = 0.1, model="MAF", batch_norm=True):
        super(test, self).__init__()
        
        if model=="MAF":
            self.nf = MAF(n_blocks, n_sensor, input_size, hidden_size, n_hidden, batch_norm=batch_norm,activation='tanh', mode='zero')
        self.attention = ScaleDotProductAttention(window_size*input_size)
    def forward(self, x, ):
        return self.test(x, ).mean()
    def test(self, x):
        x = x.unsqueeze(2).unsqueeze(3)
        full_shape = x.shape
        x = x.reshape((full_shape[0]*full_shape[1], full_shape[2], full_shape[3]))
        x = x.reshape((-1,full_shape[3]))
        log_prob = self.nf.log_prob(x, full_shape[1], full_shape[2]).reshape([full_shape[0],full_shape[1],-1])#*full_shape[1]*full_shape[2]
        log_prob = log_prob.mean(dim=1)
        return log_prob

    def locate(self, x, ):
        # x: N X K X L X D 
        x = x.unsqueeze(2).unsqueeze(3)
        full_shape = x.shape
  
        x = x.reshape((x.shape[0]*x.shape[1], x.shape[2], x.shape[3]))

        # reshappe N*K*L,H
        x = x.reshape((-1,full_shape[3]))
        a = self.nf.log_prob(x, full_shape[1], full_shape[2])#*full_shape[1]*full_shape[2]
        log_prob, z = a[0].reshape([full_shape[0],full_shape[1],-1]), a[1].reshape([full_shape[0],full_shape[1],-1])
        

        return log_prob.mean(dim=2), z.reshape((full_shape[0]* full_shape[1],-1))


